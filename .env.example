# ============================================================================
# SPCH2TXT CONFIGURATION
# ============================================================================
# Copy this file to .env and fill in your values
# All settings are optional - the app will use sensible defaults

# ============================================================================
# API ENDPOINTS
# ============================================================================

# Server API endpoint (where the Flask backend runs)
# Default: http://localhost:5001
#API_BASE_URL=http://localhost:5001

# LLM API endpoint for summarization
# Default: https://api.openai.com/v1
# Examples:
#   - OpenAI: https://api.openai.com/v1
#   - Local vLLM: http://localhost:8000/v1
#   - Ollama: http://localhost:11434/v1
#LLM_API_BASE_URL=https://api.openai.com/v1

# ============================================================================
# API KEYS & TOKENS (REQUIRED)
# ============================================================================

# OpenAI API key (REQUIRED for summarization)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your_openai_api_key_here

# HuggingFace token (REQUIRED for diarization)
# Get from: https://hf.co/settings/tokens (READ permissions)
# Accept conditions at:
#   - https://huggingface.co/pyannote/segmentation-3.0
#   - https://huggingface.co/pyannote/speaker-diarization-3.1
HUGGINGFACE_TOKEN=hf_your_huggingface_token_here

# ============================================================================
# LLM MODEL CONFIGURATION
# ============================================================================

# Model to use for meeting summarization
# Default: gpt-4
# Examples:
#   - gpt-4o (recommended)
#   - gpt-4-turbo
#   - gpt-3.5-turbo
#   - claude-3-opus-20240229
#   - llama-3-70b-instruct
#LLM_MODEL=gpt-4o

# ============================================================================
# WHISPER MODEL CONFIGURATION
# ============================================================================

# Whisper model for speech-to-text transcription
# Default: base
#
# Three-tier precedence (highest to lowest):
#   1. API request option (whisper_model in /upload options) - per-request override
#   2. This environment variable (WHISPER_MODEL) - server default
#   3. Hardcoded default ("base") - fallback
#
# OpenAI Whisper models (no prefix):
#   tiny, base, small, medium, large
#
# HuggingFace models (requires: poetry add transformers):
#   openai/whisper-tiny
#   openai/whisper-base
#   openai/whisper-small
#   openai/whisper-medium
#   openai/whisper-large-v3
#   openai/whisper-large-v3-turbo
#
# Local/Offline model (full path to snapshot with config.json):
#   For offline use, models must be pre-downloaded to HF cache.
#   Path structure: HF_HUB_CACHE/models--org--model/snapshots/HASH/
#   Example: /path/to/cache/models--openai--whisper-large-v3/snapshots/abc123def456/
#   To find hash: ls ~/.cache/huggingface/hub/models--openai--whisper-large-v3/snapshots/
#
#WHISPER_MODEL=base

# ============================================================================
# DIARIZATION MODEL CONFIGURATION
# ============================================================================

# PyAnnote model for speaker diarization
# Default: pyannote/speaker-diarization-3.1
# Requires: HUGGINGFACE_TOKEN
#
# IMPORTANT: The diarization pipeline uses multiple models internally:
#   - Main pipeline: pyannote/speaker-diarization-3.1
#   - Segmentation: pyannote/segmentation-3.0 (auto-downloaded)
#   - Embedding: pyannote/wespeaker-voxceleb-resnet34-LM (auto-downloaded)
# You only need to configure the main pipeline below.
#
# HuggingFace models (cached in ~/.cache/huggingface/ or HF_HUB_CACHE):
#   pyannote/speaker-diarization-3.1 (recommended)
#   pyannote/speaker-diarization-2.1
#
# For offline use, download while connected to internet:
#   huggingface-cli download pyannote/speaker-diarization-3.1
#   huggingface-cli download pyannote/segmentation-3.0
#   huggingface-cli download pyannote/wespeaker-voxceleb-resnet34-LM
# Then keep using the HuggingFace model name - it works from cache offline.
#
# Local/Offline model (full path to snapshot with config.json):
#   Path structure: HF_HUB_CACHE/models--org--model/snapshots/HASH/
#   Example: /cache/models--pyannote--speaker-diarization-3.1/snapshots/abc123/
#   Note: Model name uses -- instead of / in cache directory
#
#DIARIZATION_MODEL=pyannote/speaker-diarization-3.1

# ============================================================================
# OFFLINE MODE (Isolated/Air-gapped Servers)
# ============================================================================

# Force HuggingFace libraries to use only cached models (no internet access)
# Set to 1 to enable offline mode - requires models to be pre-downloaded
#HF_HUB_OFFLINE=1
#
# Custom cache location for HuggingFace models (default: ~/.cache/huggingface/)
#HF_HUB_CACHE=/path/to/models/cache
#
# Custom cache location for OpenAI Whisper models (default: ~/.cache/whisper/)
# Note: Whisper uses XDG_CACHE_HOME, creates whisper/ subdirectory inside
#XDG_CACHE_HOME=/path/to/models
#
# OFFLINE SETUP INSTRUCTIONS:
# 1. Download models while connected:
#      huggingface-cli download pyannote/speaker-diarization-3.1
#      huggingface-cli download openai/whisper-large-v3  # if using HF Whisper
#      python -c "import whisper; whisper.load_model('large')"  # if using OpenAI Whisper
#
# 2. Find snapshot hash:
#      ls ~/.cache/huggingface/hub/models--openai--whisper-large-v3/snapshots/
#      # Example output: abc123def456789
#
# 3. Copy cache to offline server:
#      scp -r ~/.cache/huggingface/ user@offline-server:/path/to/models/
#
# 4. Set environment variables:
#      HF_HUB_OFFLINE=1
#      HF_HUB_CACHE=/path/to/models
#
# 5. Use full snapshot path OR model name (if cache structure intact):
#      WHISPER_MODEL=/path/to/models/models--openai--whisper-large-v3/snapshots/abc123/
#      OR
#      WHISPER_MODEL=openai/whisper-large-v3  # works if HF_HUB_CACHE is set correctly

# ============================================================================
# NOTES
# ============================================================================
#
# Three-tier configuration precedence:
#   1. UI Settings (highest priority) - Set in app Settings page
#   2. Environment Variables - This .env file
#   3. Defaults (lowest priority) - Built into the code
#
# You can override any setting temporarily in the UI Settings page
# without modifying this file.
